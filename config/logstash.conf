input {
  http {
    host => "0.0.0.0"
    port => 5044
    codec => json_lines
  }
}

filter {
    if[header][request_method] == "GET" {
        drop{}
    }

    # 불필요한 필드를 삭제하거나 새로운 필드를 추가한다.
	mutate {
   		remove_field => ["hosts", "headers"]
      		add_field => [ "[geoip][location][lat]", "%{[geo][lat]}"  ]
		add_field => [ "[geoip][location][lon]", "%{[geo][long]}" ]
	}

    # 필드를 가공하거나 파싱하여 새로운 데이터를 만들어낼 수 있다.
    grok {
		match => { "[message][reqPath]" => "%2f%{DATA:live_channel}%2f(.*_%{DATA:live_stream}|%{DATA:live_stream})\.stream%2f(.*)\.%{DATA:content_extension}$|%2f(.*)\.%{DATA:content_extension}$" }
	}

    if [message][bytes] and [content][download][time-taken] {
		ruby {
			code => "event['download_bandwidth'] = event['message']['bytes'].to_i / (event['content']['download']['time-taken'].to_i + event['content']['download']['lastmile-rtt'].to_i + event['content']['download']['req-end-time'].to_i + event['content']['download']['req-first-byte-time'].to_i) * 1000.0 * 8 / 1000000"
		}
	}

}

output {
  stdout {
    codec => rubydebug
  }
  elasticsearch {
    hosts => ["https://localhost:9200"]
    ssl_certificate_authorities => ["/etc/logstash/config/certs/ca.crt"]
    user => "elastic"
    password => "word1234"
  }

    # Elasticserach 로 저장한다.
    # Custom template 을 사용하여 각 필드마다 원하는 데이터 타입을 미리 정의할 수 있고
    # 이렇게 해두면  Kibana 에서 좀더 효과적으로 데이터를 활용할 수 있다.
    #if "cloud_monitor" in [type] {
	#elasticsearch {
	#	index => "cloudmonitor-%{+YYYY.MM.dd}"
	#	hosts => ["[ip address]:9200"]
	#	template => "/opt/Logtank/Logstash/conf/template-cloudmonitor.json"
	#	template_overwrite => true
    #	}
    #}
}